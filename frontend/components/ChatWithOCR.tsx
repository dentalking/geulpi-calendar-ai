'use client';

import { useState, useRef, useEffect } from 'react';
import { Send, ImageIcon, Mic, MicOff } from 'lucide-react';
import { format } from 'date-fns';
import MessageContent from './MessageContent';
import ImageDropzone from './ImageDropzone';
import EventPreview from './EventPreview';
import { useProcessOcrMutation } from '@/generated/graphql';
import { handleKeyboardNavigation, KEYS, announceToScreenReader, generateAriaId } from '@/utils/accessibility';

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
  isStreaming?: boolean;
}

interface PreviewEvent {
  id: string;
  title: string;
  start: Date;
  end: Date;
  description?: string;
  isPreview: boolean;
}

interface ExtractedEvent {
  id: string;
  title: string;
  startTime: string;
  endTime: string;
  description?: string;
  location?: string;
  area?: {
    id: string;
    name: string;
    color: string;
  };
}

interface ChatWithOCRProps {
  onSendMessage?: (message: string) => void;
  onCreateEvent?: (event: any) => Promise<void>;
  messages?: Message[];
  isTyping?: boolean;
  previewEvents?: PreviewEvent[];
  onConfirmPreview?: () => void;
  onCancelPreview?: () => void;
}

export default function ChatWithOCR({
  onSendMessage,
  onCreateEvent,
  messages = [],
  isTyping = false,
  previewEvents = [],
  onConfirmPreview,
  onCancelPreview,
}: ChatWithOCRProps) {
  const [input, setInput] = useState('');
  const [showImageUpload, setShowImageUpload] = useState(false);
  const [extractedEvents, setExtractedEvents] = useState<ExtractedEvent[]>([]);
  const [isCreatingEvents, setIsCreatingEvents] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);
  const chatId = generateAriaId('chat');
  const messagesId = generateAriaId('messages');

  const [processOCR, { loading: isProcessingOCR }] = useProcessOcrMutation();

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages, extractedEvents]);

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (input.trim() && onSendMessage) {
      onSendMessage(input.trim());
      setInput('');
      announceToScreenReader('ë©”ì‹œì§€ë¥¼ ì „ì†¡í–ˆìŠµë‹ˆë‹¤', 'polite');
    }
  };

  const handleQuickAction = (action: string) => {
    onSendMessage?.(action);
    announceToScreenReader(`ë¹ ë¥¸ ì•¡ì…˜: ${action}`, 'polite');
  };

  const handleImageUploadToggle = () => {
    const newState = !showImageUpload;
    setShowImageUpload(newState);
    announceToScreenReader(
      newState ? 'ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ì´ ì—´ë ¸ìŠµë‹ˆë‹¤' : 'ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ì´ ë‹«í˜”ìŠµë‹ˆë‹¤', 
      'polite'
    );
  };

  const handleImageAccepted = async (file: File, base64: string) => {
    try {
      const { data } = await processOCR({
        variables: { imageBase64: base64 },
      });

      if (data?.processOCR) {
        const { message, events } = data.processOCR;

        // Add AI response message
        const aiMessage: Message = {
          id: Date.now().toString(),
          content: message || 'I processed the image and found the following events:',
          role: 'assistant',
          timestamp: new Date(),
        };

        // Convert events to preview format
        const convertedEvents: ExtractedEvent[] = events.map((event) => ({
          id: event.id,
          title: event.title,
          startTime: event.startTime,
          endTime: event.endTime,
          area: event.area,
        }));

        setExtractedEvents(convertedEvents);

        // Add user message showing they uploaded an image
        const userMessage: Message = {
          id: (Date.now() - 1).toString(),
          content: `ğŸ“· Uploaded image: ${file.name}`,
          role: 'user',
          timestamp: new Date(),
        };

        // Simulate adding messages (in real app, this would be handled by parent)
        if (onSendMessage) {
          onSendMessage(`Uploaded image for OCR processing: ${file.name}`);
        }
      }
    } catch (error) {
      console.error('Failed to process OCR:', error);
      
      const errorMessage: Message = {
        id: Date.now().toString(),
        content: 'Sorry, I failed to process the image. Please try again.',
        role: 'assistant',
        timestamp: new Date(),
      };
    }
  };

  const handleConfirmEvents = async () => {
    if (!onCreateEvent || extractedEvents.length === 0) return;

    setIsCreatingEvents(true);

    try {
      for (const event of extractedEvents) {
        await onCreateEvent({
          title: event.title,
          startTime: event.startTime,
          endTime: event.endTime,
          description: event.description,
          area: event.area,
        });
      }

      setExtractedEvents([]);

      // Add confirmation message
      const confirmMessage: Message = {
        id: Date.now().toString(),
        content: `âœ… Successfully added ${extractedEvents.length} event${
          extractedEvents.length > 1 ? 's' : ''
        } to your calendar!`,
        role: 'assistant',
        timestamp: new Date(),
      };
    } catch (error) {
      console.error('Failed to create events:', error);
      
      const errorMessage: Message = {
        id: Date.now().toString(),
        content: 'âŒ Failed to add events to calendar. Please try again.',
        role: 'assistant',
        timestamp: new Date(),
      };
    } finally {
      setIsCreatingEvents(false);
    }
  };

  const handleCancelEvents = () => {
    setExtractedEvents([]);
    
    const cancelMessage: Message = {
      id: Date.now().toString(),
      content: 'âŒ Cancelled adding events from the image.',
      role: 'assistant',
      timestamp: new Date(),
    };
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      const audioChunks: BlobPart[] = [];

      recorder.ondataavailable = (event) => {
        audioChunks.push(event.data);
      };

      recorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
        // In a real app, you would send this to a speech-to-text API
        // For now, we'll just simulate it
        stream.getTracks().forEach(track => track.stop());
        setIsRecording(false);
        
        // Simulate transcription
        announceToScreenReader('ìŒì„± ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤', 'polite');
        // In real app: const transcription = await transcribeAudio(audioBlob);
        // onSendMessage?.(transcription);
      };

      recorder.start();
      setMediaRecorder(recorder);
      setIsRecording(true);
      announceToScreenReader('ìŒì„± ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤', 'polite');
    } catch (error) {
      console.error('Failed to start recording:', error);
      announceToScreenReader('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤', 'assertive');
    }
  };

  const stopRecording = () => {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop();
      setIsRecording(false);
    }
  };

  return (
    <div 
      className="flex flex-col h-full" 
      role="region" 
      aria-label="ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"
      id={chatId}
    >
      <div 
        className="flex-1 overflow-y-auto p-4 space-y-4"
        role="log"
        aria-live="polite" 
        aria-label="ëŒ€í™” ë‚´ìš©"
        id={messagesId}
      >
        {messages.length === 0 ? (
          <div className="text-center text-gray-500 mt-8" role="status">
            <p className="text-lg font-medium mb-2">ì¼ì • ê´€ë¦¬ ë„ìš°ë¯¸</p>
            <p className="text-sm">
              ì¼ì •ì„ ì¶”ê°€í•˜ê±°ë‚˜ ë³€ê²½í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
              <br />
              ìì—°ìŠ¤ëŸ½ê²Œ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.
            </p>
          </div>
        ) : (
          <>
            {messages.map((message) => (
              <div
                key={message.id}
                className={`flex ${
                  message.role === 'user' ? 'justify-end' : 'justify-start'
                }`}
                role="article"
                aria-label={`${message.role === 'user' ? 'ì‚¬ìš©ì' : 'ì–´ì‹œìŠ¤í„´íŠ¸'} ë©”ì‹œì§€`}
              >
                <div
                  className={`max-w-[80%] rounded-lg px-4 py-2 ${
                    message.role === 'user'
                      ? 'bg-blue-500 text-white'
                      : message.isStreaming
                      ? 'bg-blue-50 text-gray-900 border border-blue-200'
                      : 'bg-gray-100 text-gray-900'
                  }`}
                  data-testid={message.role === 'assistant' ? 'ai-response' : 'user-message'}
                >
                  <MessageContent content={message.content} />
                  <p className="text-xs opacity-70 mt-1" aria-label={`ì „ì†¡ ì‹œê°„: ${format(message.timestamp, 'HH:mm')}`}>
                    {format(message.timestamp, 'HH:mm')}
                    {message.isStreaming && (
                      <span className="ml-1 animate-pulse" aria-label="ë©”ì‹œì§€ ì „ì†¡ ì¤‘">â€¢â€¢â€¢</span>
                    )}
                  </p>
                </div>
              </div>
            ))}

            {/* Typing indicator */}
            {(isTyping || isProcessingOCR || isRecording) && (
              <div className="flex justify-start">
                <div className="bg-gray-100 rounded-lg px-4 py-2" data-testid={isRecording ? "voice-processing" : "ai-typing-indicator"}>
                  <div className="flex items-center space-x-1">
                    <div className="flex space-x-1">
                      <div
                        className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"
                        style={{ animationDelay: '0ms' }}
                      ></div>
                      <div
                        className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"
                        style={{ animationDelay: '150ms' }}
                      ></div>
                      <div
                        className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"
                        style={{ animationDelay: '300ms' }}
                      ></div>
                    </div>
                    <span className="text-xs text-gray-500 ml-2">
                      {isRecording ? 'ìŒì„± ë…¹ìŒ ì¤‘...' : isProcessingOCR ? 'ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...' : 'ìƒê° ì¤‘...'}
                    </span>
                  </div>
                </div>
              </div>
            )}

            {/* Image upload area */}
            {showImageUpload && (
              <div className="mt-4">
                <ImageDropzone
                  onImageAccepted={handleImageAccepted}
                  isProcessing={isProcessingOCR}
                />
              </div>
            )}

            {/* Event preview */}
            {extractedEvents.length > 0 && (
              <EventPreview
                events={extractedEvents}
                onConfirm={handleConfirmEvents}
                onCancel={handleCancelEvents}
                isLoading={isCreatingEvents}
              />
            )}

            {/* Preview confirmation buttons for existing events */}
            {previewEvents.length > 0 && (
              <div className="flex justify-center space-x-2 mt-2">
                <button
                  onClick={onConfirmPreview}
                  className="px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 transition-colors flex items-center space-x-1"
                >
                  <span>âœ…</span>
                  <span>í™•ì¸</span>
                </button>
                <button
                  onClick={onCancelPreview}
                  className="px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 transition-colors flex items-center space-x-1"
                >
                  <span>âŒ</span>
                  <span>ì·¨ì†Œ</span>
                </button>
              </div>
            )}
          </>
        )}
        <div ref={messagesEndRef} />
      </div>

      {/* Quick action buttons */}
      <div className="px-4 py-2 border-t bg-gray-50" role="toolbar" aria-label="ë¹ ë¥¸ ì•¡ì…˜">
        <div className="flex flex-wrap gap-2">
          <button
            onClick={() => handleQuickAction('ì˜¤ëŠ˜ ì¼ì • ë³´ì—¬ì¤˜')}
            className="px-3 py-1 text-xs bg-white border rounded-full hover:bg-gray-100 transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500"
            aria-label="ì˜¤ëŠ˜ ì¼ì • ì¡°íšŒí•˜ê¸°"
            type="button"
          >
            ğŸ“… ì˜¤ëŠ˜ ì¼ì •
          </button>
          <button
            onClick={() => handleQuickAction('ë‚´ì¼ ì¼ì • ë³´ì—¬ì¤˜')}
            className="px-3 py-1 text-xs bg-white border rounded-full hover:bg-gray-100 transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500"
            aria-label="ë‚´ì¼ ì¼ì • ì¡°íšŒí•˜ê¸°"
            type="button"
          >
            ğŸ“… ë‚´ì¼ ì¼ì •
          </button>
          <button
            onClick={() => handleQuickAction('ì´ë²ˆì£¼ ì¼ì • ë³´ì—¬ì¤˜')}
            className="px-3 py-1 text-xs bg-white border rounded-full hover:bg-gray-100 transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500"
            aria-label="ì´ë²ˆì£¼ ì¼ì • ì¡°íšŒí•˜ê¸°"
            type="button"
          >
            ğŸ“… ì´ë²ˆì£¼ ì¼ì •
          </button>
          <button
            data-testid="voice-input-button"
            onClick={startRecording}
            className="px-3 py-1 text-xs border rounded-full transition-colors flex items-center gap-1 focus:outline-none focus:ring-2 focus:ring-blue-500 bg-white hover:bg-gray-100"
            aria-label="ìŒì„±ìœ¼ë¡œ ì¼ì • ì¶”ê°€í•˜ê¸°"
            type="button"
          >
            <Mic className="w-3 h-3" aria-hidden="true" />
            ğŸ¤ ìŒì„± ì…ë ¥
          </button>
          <button
            onClick={handleImageUploadToggle}
            className={`px-3 py-1 text-xs border rounded-full transition-colors flex items-center gap-1 focus:outline-none focus:ring-2 focus:ring-blue-500 ${
              showImageUpload
                ? 'bg-blue-100 border-blue-300 text-blue-700'
                : 'bg-white hover:bg-gray-100'
            }`}
            aria-label={showImageUpload ? 'ì´ë¯¸ì§€ ì—…ë¡œë“œ ë‹«ê¸°' : 'ì´ë¯¸ì§€ ì—…ë¡œë“œ ì—´ê¸°'}
            aria-expanded={showImageUpload}
            type="button"
          >
            <ImageIcon className="w-3 h-3" aria-hidden="true" />
            ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ
          </button>
        </div>
      </div>

      <form onSubmit={handleSubmit} className="p-4 border-t" role="search" aria-label="ë©”ì‹œì§€ ì…ë ¥">
        <div className="flex gap-2">
          <input
            ref={inputRef}
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="ì¼ì •ì„ ë§ì”€í•´ì£¼ì„¸ìš”..."
            className="flex-1 px-4 py-2 border rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
            disabled={isTyping || isProcessingOCR || isRecording}
            aria-label="ë©”ì‹œì§€ ì…ë ¥"
            aria-describedby="input-help"
            aria-busy={isTyping || isProcessingOCR}
            data-testid="chat-input"
          />
          <button
            type="button"
            onClick={isRecording ? stopRecording : startRecording}
            className={`p-2 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500 transition-colors ${
              isRecording 
                ? 'bg-red-500 text-white hover:bg-red-600' 
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
            aria-label={isRecording ? 'ìŒì„± ë…¹ìŒ ì¤‘ì§€' : 'ìŒì„± ë…¹ìŒ ì‹œì‘'}
            data-testid="voice-record-button"
          >
            {isRecording ? (
              <MicOff className="w-5 h-5" aria-hidden="true" />
            ) : (
              <Mic className="w-5 h-5" aria-hidden="true" />
            )}
          </button>
          <button
            type="submit"
            disabled={!input.trim() || isTyping || isProcessingOCR || isRecording}
            className="p-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed focus:outline-none focus:ring-2 focus:ring-blue-500"
            aria-label="ë©”ì‹œì§€ ì „ì†¡"
          >
            <Send className="w-5 h-5" aria-hidden="true" />
          </button>
        </div>

        {/* Suggested input examples */}
        <div id="input-help" className="mt-2 text-xs text-gray-500" role="note">
          ì˜ˆì‹œ: &quot;ë‚´ì¼ ì˜¤í›„ 3ì‹œì— íšŒì˜ ì¶”ê°€í•´ì¤˜&quot;, &quot;ì˜¤ëŠ˜ ì¼ì • ëª¨ë‘ ë³´ì—¬ì¤˜&quot;, ë˜ëŠ” ì¼ì •ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
        </div>
      </form>
    </div>
  );
}