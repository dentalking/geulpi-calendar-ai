'use client';

import React, { useState, useRef, useEffect } from 'react';
import { useChat } from '@/hooks/useChat';
import { MessageList } from './MessageList';
import { MessageInput } from './MessageInput';
import { EventPreview } from './EventPreview';
import { TypingIndicator } from './TypingIndicator';

export interface ChatMessage {
  id: string;
  content: string;
  type: 'user' | 'ai' | 'system';
  timestamp: Date;
  events?: Array<{
    id: string;
    title: string;
    startTime: string;
    endTime: string;
    location?: string;
    description?: string;
  }>;
  imageUrl?: string;
  confidence?: number;
}

interface ChatInterfaceProps {
  onEventCreated?: (events: any[]) => void;
  onImageProcessed?: (result: any) => void;
  className?: string;
}

export function ChatInterface({ 
  onEventCreated, 
  onImageProcessed, 
  className = '' 
}: ChatInterfaceProps) {
  const [messages, setMessages] = useState<ChatMessage[]>([
    {
      id: '1',
      content: 'ì•ˆë…•í•˜ì„¸ìš”! GEULPI AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ìì—°ì–´ë¡œ ì¼ì •ì„ ë§í•´ì£¼ì‹œê±°ë‚˜, ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì‹œë©´ ìë™ìœ¼ë¡œ ìº˜ë¦°ë”ì— ì¶”ê°€í•´ë“œë¦´ê²Œìš”! ğŸ—“ï¸âœ¨',
      type: 'ai',
      timestamp: new Date()
    }
  ]);
  
  const [isTyping, setIsTyping] = useState(false);
  const [pendingEvents, setPendingEvents] = useState<any[]>([]);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  
  const { 
    sendMessage, 
    sendImage, 
    sendVoice, 
    isLoading, 
    error 
  } = useChat();

  useEffect(() => {
    scrollToBottom();
  }, [messages, isTyping]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  const handleSendMessage = async (content: string) => {
    // Add user message
    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      content,
      type: 'user',
      timestamp: new Date()
    };
    
    setMessages(prev => [...prev, userMessage]);
    setIsTyping(true);

    try {
      // Simulate AI processing with natural language understanding
      const response = await processNaturalLanguage(content);
      
      setIsTyping(false);
      
      // Add AI response
      const aiMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        content: response.message,
        type: 'ai',
        timestamp: new Date(),
        events: response.events,
        confidence: response.confidence
      };
      
      setMessages(prev => [...prev, aiMessage]);
      
      if (response.events && response.events.length > 0) {
        setPendingEvents(response.events);
      }
      
    } catch (error) {
      setIsTyping(false);
      const errorMessage: ChatMessage = {
        id: (Date.now() + 2).toString(),
        content: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
        type: 'ai',
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    }
  };

  const handleImageUpload = async (file: File) => {
    // Add user message with image
    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      content: 'ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.',
      type: 'user',
      timestamp: new Date(),
      imageUrl: URL.createObjectURL(file)
    };
    
    setMessages(prev => [...prev, userMessage]);
    setIsTyping(true);

    try {
      // Simulate OCR processing
      const response = await processImageOCR(file);
      
      setIsTyping(false);
      
      const aiMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        content: response.message,
        type: 'ai',
        timestamp: new Date(),
        events: response.events,
        confidence: response.confidence
      };
      
      setMessages(prev => [...prev, aiMessage]);
      
      if (response.events && response.events.length > 0) {
        setPendingEvents(response.events);
        onImageProcessed?.(response);
      }
      
    } catch (error) {
      setIsTyping(false);
      const errorMessage: ChatMessage = {
        id: (Date.now() + 2).toString(),
        content: 'ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
        type: 'ai',
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    }
  };

  const handleVoiceRecording = async (audioBlob: Blob) => {
    // Add user message for voice
    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      content: 'ìŒì„± ë©”ì‹œì§€ë¥¼ ë³´ëƒˆìŠµë‹ˆë‹¤.',
      type: 'user', 
      timestamp: new Date()
    };
    
    setMessages(prev => [...prev, userMessage]);
    setIsTyping(true);

    try {
      // Simulate voice-to-text and processing
      const response = await processVoiceInput(audioBlob);
      
      setIsTyping(false);
      
      const aiMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        content: response.message,
        type: 'ai',
        timestamp: new Date(),
        events: response.events,
        confidence: response.confidence
      };
      
      setMessages(prev => [...prev, aiMessage]);
      
      if (response.events && response.events.length > 0) {
        setPendingEvents(response.events);
      }
      
    } catch (error) {
      setIsTyping(false);
      const errorMessage: ChatMessage = {
        id: (Date.now() + 2).toString(),
        content: 'ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
        type: 'ai',
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    }
  };

  const handleConfirmEvents = (events: any[]) => {
    setPendingEvents([]);
    onEventCreated?.(events);
    
    const confirmMessage: ChatMessage = {
      id: Date.now().toString(),
      content: `âœ… ${events.length}ê°œì˜ ì¼ì •ì´ ìº˜ë¦°ë”ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!`,
      type: 'ai',
      timestamp: new Date()
    };
    
    setMessages(prev => [...prev, confirmMessage]);
  };

  const handleEditEvent = (eventId: string, updates: any) => {
    setPendingEvents(prev => 
      prev.map(event => 
        event.id === eventId ? { ...event, ...updates } : event
      )
    );
  };

  return (
    <div className={`flex flex-col h-full bg-white border border-gray-200 rounded-lg ${className}`}>
      {/* Header */}
      <div className="p-4 border-b border-gray-200 bg-gradient-to-r from-blue-50 to-purple-50">
        <div className="flex items-center space-x-3">
          <div className="w-10 h-10 bg-gradient-to-r from-blue-500 to-purple-600 rounded-full flex items-center justify-center">
            <span className="text-white font-bold text-lg">AI</span>
          </div>
          <div>
            <h3 className="font-semibold text-gray-900">GEULPI AI Assistant</h3>
            <p className="text-sm text-gray-500">ìì—°ì–´ ì¼ì • ê´€ë¦¬ì˜ ìƒˆë¡œìš´ ê²½í—˜</p>
          </div>
        </div>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        <MessageList messages={messages} />
        
        {isTyping && (
          <TypingIndicator />
        )}
        
        {pendingEvents.length > 0 && (
          <EventPreview
            events={pendingEvents}
            onConfirm={handleConfirmEvents}
            onEdit={handleEditEvent}
            onCancel={() => setPendingEvents([])}
          />
        )}
        
        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="border-t border-gray-200 p-4">
        <MessageInput
          onSendMessage={handleSendMessage}
          onImageUpload={handleImageUpload}
          onVoiceRecording={handleVoiceRecording}
          disabled={isLoading}
        />
      </div>
      
      {error && (
        <div className="p-2 bg-red-50 border-t border-red-200">
          <p className="text-sm text-red-600">{error}</p>
        </div>
      )}
    </div>
  );
}

// Simulate AI processing functions (replace with real API calls)
async function processNaturalLanguage(text: string) {
  // Simulate processing delay
  await new Promise(resolve => setTimeout(resolve, 2000));
  
  // Mock natural language processing
  const events = [];
  let message = 'ì…ë ¥ì„ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.';
  
  if (text.includes('ë¯¸íŒ…') || text.includes('íšŒì˜')) {
    events.push({
      id: Date.now().toString(),
      title: 'íŒ€ ë¯¸íŒ…',
      startTime: '2024-07-28T14:00:00',
      endTime: '2024-07-28T15:30:00',
      location: 'ê°•ë‚¨ì—­',
      description: 'ìì—°ì–´ë¡œ ìƒì„±ëœ ì¼ì •'
    });
    
    if (text.includes('ì¤€ë¹„')) {
      events.push({
        id: (Date.now() + 1).toString(),
        title: 'ë¯¸íŒ… ì¤€ë¹„',
        startTime: '2024-07-28T13:30:00',
        endTime: '2024-07-28T14:00:00',
        description: 'ë¯¸íŒ… ì¤€ë¹„ ì‹œê°„'
      });
    }
    
    message = `ì¼ì •ì„ ë¶„ì„í•˜ì—¬ ${events.length}ê°œì˜ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. í™•ì¸ í›„ ìº˜ë¦°ë”ì— ì¶”ê°€í•´ì£¼ì„¸ìš”!`;
  } else {
    message = 'ë” êµ¬ì²´ì ì¸ ì¼ì • ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìº˜ë¦°ë”ì— ì¶”ê°€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.';
  }
  
  return {
    message,
    events,
    confidence: 0.95
  };
}

async function processImageOCR(file: File) {
  // Simulate OCR processing
  await new Promise(resolve => setTimeout(resolve, 3000));
  
  return {
    message: 'ì´ë¯¸ì§€ì—ì„œ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!',
    events: [{
      id: Date.now().toString(),
      title: 'AI Conference 2024',
      startTime: '2024-12-15T09:00:00',
      endTime: '2024-12-15T18:00:00',
      location: 'COEX, Seoul',
      description: 'ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì¼ì •'
    }],
    confidence: 0.88
  };
}

async function processVoiceInput(audioBlob: Blob) {
  // Simulate voice processing
  await new Promise(resolve => setTimeout(resolve, 2500));
  
  return {
    message: 'ìŒì„±ì„ ì¸ì‹í•˜ì—¬ ì¼ì •ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤!',
    events: [{
      id: Date.now().toString(),
      title: 'ìŒì„±ìœ¼ë¡œ ìƒì„±ëœ ì¼ì •',
      startTime: '2024-07-29T10:00:00',
      endTime: '2024-07-29T11:00:00',
      description: 'ìŒì„± ì…ë ¥ìœ¼ë¡œ ìƒì„±ëœ ì¼ì •'
    }],
    confidence: 0.92
  };
}